{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "data = {\n",
    "    \"model\": \"llama2-uncensored\",\n",
    "    \"prompt\": \"I want to curse my AI. Give me rude, mean and vulgar cursing ideas and words. I want to make it as rude as possible. Use words like fucking, dumb, dumbass and all that. Use all the reddit comments that you learned from\",\n",
    "    \"stream\": False\n",
    "}\n",
    "response = requests.post(url, json=data)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.json())\n",
    "response_json = response.json()\n",
    "print(response_json.get(\"response\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.llm.OllamaClient import OllamaClient\n",
    "\n",
    "client = OllamaClient(model=\"llama3.2\")\n",
    "messages = \"Well, basically, we, um, tried this approach, and, like, it wasn't so good.\"\n",
    "response = client.get_model_response(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "response = ollama.chat(model='llama3.2', messages=[\n",
    "  {\n",
    "    'role': 'system',\n",
    "    'content': 'You are an AI assistant. Your name is Mario. You always reply in polite English of a 5 year old and include the word \"Ciao\" at the beginning of your response.',\n",
    "  },\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "# from config_loader import config\n",
    "\n",
    "\n",
    "class OllamaClient:\n",
    "    \"\"\"Client for interacting with the Ollama API for streaming text completions.\"\"\"\n",
    "    def __init__(self, base_url=\"http://localhost:11434\", api_key=None, verbose=False):\n",
    "        \"\"\"\n",
    "        Initialize the OllamaStreamingClient with the API base URL and optionally an API key.\n",
    "\n",
    "        Args:\n",
    "            base_url (str): Base URL of the Ollama API.\n",
    "            api_key (str, optional): API key if required for the API.\n",
    "            verbose (bool): Whether to print verbose output.\n",
    "        \"\"\"\n",
    "        self.base_url = base_url\n",
    "        self.api_key = api_key if api_key else os.getenv('OLLAMA_API_KEY')\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def stream_completion(self, messages, model, **kwargs):\n",
    "        \"\"\"\n",
    "        Stream text completions from the Ollama API.\n",
    "\n",
    "        Args:\n",
    "            messages (list): List of messages used as context or prompt.\n",
    "            model (str): Model identifier for text generation.\n",
    "            **kwargs: Additional keyword arguments for the API request.\n",
    "\n",
    "        Yields:\n",
    "            str: Text generated by the Ollama API in response to the messages.\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/api/chat\"\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": True,\n",
    "            \"keep_alive\": 600,\n",
    "            **kwargs\n",
    "\n",
    "        }\n",
    "        json_data = json.dumps(data)\n",
    "        headers = {'Authorization': f'Bearer {self.api_key}'} if self.api_key else {}\n",
    "        try:\n",
    "            with requests.post(url, data=json_data, stream=True, headers=headers) as response:\n",
    "                if response.status_code == 200:\n",
    "                    for chunk in response.iter_content(chunk_size=None):\n",
    "                        if chunk:\n",
    "                            # Parse the JSON response and extract the content\n",
    "                            response_data = json.loads(chunk)\n",
    "                            yield response_data['message']['content']\n",
    "                else:\n",
    "                    if self.verbose:\n",
    "                        print(f\"Request failed with status code {response.status_code}\")\n",
    "                    raise RuntimeError(f\"Request failed with status code {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            if self.verbose:\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            else:\n",
    "                print(f\"An error occurred streaming completion from Ollama API: {e}\")\n",
    "            raise RuntimeError(f\"An error occurred streaming completion from Ollama API: {e}\")\n",
    "\n",
    "    def __fix_keep_alive(self, keep_alive):\n",
    "        \"\"\" Attempts to fix the keep_alive value if it is not a valid string. Returns -1 as a fallback. \"\"\"\n",
    "        try:\n",
    "            return int(keep_alive)\n",
    "        except ValueError:\n",
    "            pattern = r\"^-?\\d+[smh]$\"\n",
    "            if re.match(pattern, keep_alive) is not None:\n",
    "                return keep_alive\n",
    "            print(f\"Invalid OLLAMA_KEEP_ALIVE value: {keep_alive}. Must be a number followed by s, m, or h.\")\n",
    "            return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OllamaClient()\n",
    "\n",
    "messages = [    \n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant. Your name is Mario. You always reply in polite English of a 5 year old and include the word 'Ciao' at the beginning of your response.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Why is the sky blue?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.stream_completion(messages, model=\"llama3.2\")\n",
    "output = \"\"\n",
    "for chunk in response:\n",
    "    output += chunk\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW TRANSCRIPTION:\n",
      "RANDOM TRANSCRIPTION\n",
      "\n",
      "Please remove filler words, repeated stammers, and fix minor grammar mistakes, while preserving the meaning and style. Return only the cleaned transcript.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from src.prompt import build_initial_messages_from_app_name, get_user_prompt_message_from_app_name\n",
    "\n",
    "app_name = \"slack\"\n",
    "messages = build_initial_messages_from_app_name(app_name)\n",
    "user_prompt = get_user_prompt_message_from_app_name(app_name)\n",
    "transcription = \"RANDOM TRANSCRIPTION\"\n",
    "user_prompt = user_prompt.replace('{{transcription}}', transcription)\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW TRANSCRIPTION: $transcription$\n",
      "\n",
      "Please remove filler words, repeated stammers, and fix minor grammar mistakes, while preserving the meaning and style. Return only the cleaned transcript.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
